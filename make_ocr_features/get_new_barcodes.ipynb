{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "import io\n",
    "def text_cleaner(text):\n",
    "  dont_take = [\"kj\", \"kcal\", \"kj\", \"total\", \"free\", \"net\", \"ingredients\", \"ingredient\", \"et\", \"de\", \"fat\", \"mg\", \"cg\", \"g\", \"kg\", \"ml\", \"cl\", \"l\", \"kl\", \"per\", \"pour\", \"valeur\", \"or\", \"le\", \"la\", \"dont\", \"consommer\", \"poids\", \"net\", \"www\", \"com\", \"which\", \"of\", \"wt\"]\n",
    "  text_cleaned = text.replace(\"\\n\", \" \") #remove line breaks\n",
    "  text_cleaned = re.sub(\"\\S*(www\\.|\\.com|\\.net|\\.fr|\\.co\\.uk|\\.org)\\S*\", \"\", text_cleaned) #remove websites\n",
    "  #text_cleaned = re.sub(\"[^A-Za-z0-9 \\-àâäéèêëîïôöùûüÿçÂÊÎÔÛÄËÏÖÜÀÆæÇÉÈŒœÙ]\", \" \", text_cleaned) #keep alphanum and accents\n",
    "  text_cleaned = re.sub(\"\\w*([0-9]{0,}[,|\\.]{0,}[0-9])\\w*\", \" \", text_cleaned) #remove measurements \n",
    "  text_cleaned = re.sub(r\"\\b([a-zA-Z]{1})\\b\", \" \", text_cleaned) # remove isolated letters ex --> g g g g g\n",
    "  text_cleaned = re.sub(\"( +- +)\", \" \", text_cleaned)\n",
    "  text_cleaned = re.sub(r\"[\\·|/|\\-|\\\\|(|)|\\+|\\*|\\[|\\]|™|ᴿˣ|\\*|\\—|\\^|\\\"|®|>|<|″|\\||\\&|\\#|\\,|\\;|⭐|\\xa0|\\?|\\%|\\'|©|\\@|\\$|\\€|\\:|\\}|\\{|\\°]\", \" \", text_cleaned)\n",
    "  text_cleaned = re.sub(r\" +\", \" \", text_cleaned) # remove multiple spaces\n",
    "\n",
    "  text_cleaned = \" \".join([w for w in text_cleaned.split() if (w.isalpha() and w.lower() not in dont_take)])\n",
    "  return text_cleaned\n",
    "  \n",
    "def make_barcode(x):\n",
    "    x = str(x)\n",
    "    return \"{}/{}/{}/{}\".format(x[:3], x[3:6], x[6:9], x[9:])\n",
    "\n",
    "def make_link_from_barcode(barcode, df, file = \"image\", keys = None):\n",
    "    if keys is None:\n",
    "        keys = df.loc[df[\"code\"]==barcode, \"keys\"].values[0]\n",
    "    if isinstance(keys, str):\n",
    "        keys = eval(keys)\n",
    "    elif isinstance(keys, list):\n",
    "        pass\n",
    "\n",
    "    links = []\n",
    "    if file == \"image\": file = \"jpg\"\n",
    "    if file == \"json\": file = \"json\"\n",
    "    barcode_with_slash = make_barcode(barcode)\n",
    "    for key in keys:\n",
    "        link = \"https://world.openfoodfacts.org/images/products/{}/{}.{}\".format(barcode_with_slash, key,file)\n",
    "        links.append(link)\n",
    "    return links\n",
    "\n",
    "def show_images(links):\n",
    "    for link in links:\n",
    "        response = requests.get(link)\n",
    "        image_bytes = io.BytesIO(response.content)\n",
    "        img = Image.open(image_bytes)\n",
    "        img.show()\n",
    "\n",
    "\n",
    "\n",
    "def show_images_from_barcode(barcode, df, keys = None):\n",
    "    links = make_link_from_barcode(barcode, df=df, keys = keys)\n",
    "    show_images(links)\n",
    "\n",
    "import math\n",
    "import requests\n",
    "def get_score_from_verticles(txt_annotations:dict):\n",
    "    \n",
    "    txt = txt_annotations[\"description\"]\n",
    "    len_text = len(txt)\n",
    "    y_min = math.inf\n",
    "    y_max = -math.inf\n",
    "    x_min = math.inf\n",
    "    x_max = -math.inf\n",
    "\n",
    "    verticles = txt_annotations['boundingPoly']['vertices']\n",
    "    for coords in verticles:\n",
    "        if 'y' in coords:\n",
    "            y_min = min(coords['y'], y_min)\n",
    "            y_max = max(coords['y'], y_max)\n",
    "        if 'x' in coords:\n",
    "            x_min = min(coords['x'], x_min)\n",
    "            x_max = max(coords['x'], x_max)\n",
    "    volume = abs(x_max-x_min) * abs(y_max-y_min)\n",
    "    score = volume/len_text\n",
    "    return score,txt\n",
    "\n",
    "def get_n_most_important_words(results, word_count_limit = 10):\n",
    "    to_keep = {}\n",
    "    for items in results:\n",
    "        words = items[1]\n",
    "        for word in text_cleaner(words).split():\n",
    "            if word not in to_keep:\n",
    "                to_keep[word.lower()] = word\n",
    "                if len(to_keep) == word_count_limit:\n",
    "                    return \" \".join(to_keep.values())\n",
    "    return \" \".join(to_keep.values())      \n",
    "\n",
    "\n",
    "def get_big_words_from_image(barcode, df, keys = None):\n",
    "    #clear_output()\n",
    "    #print(barcode)\n",
    "    texts = []\n",
    "    links = make_link_from_barcode(barcode, df, file = \"json\", keys = keys)\n",
    "    for link in links:\n",
    "        try:\n",
    "            response = urllib.request.urlopen(link)\n",
    "            js = json.loads(response.read())\n",
    "            txt_annotations = js['responses'][0]['textAnnotations']\n",
    "            results = sorted([get_score_from_verticles(txt_a)  for txt_a in txt_annotations], reverse = True)\n",
    "            text = get_n_most_important_words(results, word_count_limit = 10)\n",
    "        except:\n",
    "            text = \"\"\n",
    "        texts.append(text)\n",
    "    return \" \".join(texts)\n",
    "\n",
    "def get_big_words_from_image_clean(barcode, df, keys = None):\n",
    "    return text_cleaner(get_big_words_from_image(barcode, df, keys = keys))\n",
    "\n",
    "def get_big_words_from_txt_annotations(txt_annotations):\n",
    "    try:\n",
    "        results = sorted([get_score_from_verticles(txt_a) for txt_a in txt_annotations], reverse = True)\n",
    "        text = get_n_most_important_words(results, word_count_limit = 10)\n",
    "    except:\n",
    "        text = \"\"\n",
    "    return text\n",
    "\n",
    "def get_code_from_link(link):\n",
    "    code_with_bars = link.split(\"/\",5)[-1].rsplit(\"/\",1)[0]\n",
    "    code = re.sub(\"/\", \"\", code_with_bars)\n",
    "    return code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "233003"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle \n",
    "path = \"big_words_dict.pkl\"\n",
    "with open(path, 'rb') as file:\n",
    "      \n",
    "    big_words_dict = pickle.load(file)\n",
    "df = pd.read_pickle(\"dataset.pkl\")\n",
    "codes_to_fetch = [code for code in df[\"code\"] if code not in big_words_dict]\n",
    "len(codes_to_fetch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_link_from_barcode(barcode, keys, file = \"image\"):\n",
    "    if isinstance(keys, str):\n",
    "        keys = eval(keys)\n",
    "    elif isinstance(keys, list):\n",
    "        pass\n",
    "\n",
    "    links = []\n",
    "    if file == \"image\": file = \"jpg\"\n",
    "    if file == \"json\": file = \"json\"\n",
    "    barcode_with_slash = make_barcode(barcode)\n",
    "    for key in keys:\n",
    "        link = \"https://world.openfoodfacts.org/images/products/{}/{}.{}\".format(barcode_with_slash, key,file)\n",
    "        links.append(link)\n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 233003/233003 [00:00<00:00, 444628.49it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "links_list = []\n",
    "for barcode, keys in zip(tqdm(codes_to_fetch), df.loc[df[\"code\"].isin(codes_to_fetch), \"keys\"]):\n",
    "    links = make_link_from_barcode(barcode, keys, file = \"json\")\n",
    "    links_list.append(links)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "521462"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "links_to_fetch = [link for list_ in links_list for link in list_]\n",
    "len(links_to_fetch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/az/miniforge3/envs/env_tf/lib/python3.9/asyncio/tasks.py:118: RuntimeWarning: coroutine 'download_all' was never awaited\n",
      "  super().__init__(loop=loop)\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download 521462 links in 2622.443365097046 seconds\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import time \n",
    "import aiohttp\n",
    "from aiohttp.client import ClientSession\n",
    "my_conn = aiohttp.TCPConnector(limit=10)\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "async def download_link(url:str,session:ClientSession):\n",
    "    async with session.get(url) as response:\n",
    "        result = await response.text()\n",
    "        return result\n",
    "        #print(f'Read {len(result)} from {url}')\n",
    "\n",
    "async def download_all(urls:list):\n",
    "    my_conn = aiohttp.TCPConnector(limit=10)\n",
    "    async with aiohttp.ClientSession(connector=my_conn) as session:\n",
    "        tasks = []\n",
    "        for url in urls:\n",
    "            task = asyncio.ensure_future(download_link(url=url,session=session))\n",
    "            tasks.append(task)\n",
    "        results = await asyncio.gather(*tasks,return_exceptions=True) # the await must be nest inside of the session\n",
    "        return results \n",
    "\n",
    "url_list = links_to_fetch\n",
    "\n",
    "start = time.time()\n",
    "results = asyncio.run(download_all(url_list))\n",
    "end = time.time()\n",
    "print(f'download {len(url_list)} links in {end - start} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 521462/521462 [05:55<00:00, 1465.50it/s]  \n"
     ]
    }
   ],
   "source": [
    "\"\"\"takes approx 6min\"\"\"\n",
    "for link, json_string in zip(tqdm(links_to_fetch), results):\n",
    "    barcode = get_code_from_link(link)\n",
    "    if str(json_string) != \"\":\n",
    "        js = json.loads(json_string)\n",
    "        txt_annotations = js['responses'][0]['textAnnotations']\n",
    "        big_words = get_big_words_from_txt_annotations(txt_annotations)\n",
    "        if barcode in dic:\n",
    "            if len(dic[barcode]) <= 500:\n",
    "                dic[barcode] = \" \" + str(big_words)\n",
    "        else:\n",
    "            dic[barcode] = str(big_words)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('new_barcodes_dict.pkl', 'wb') as file:\n",
    "    # A new file will be created\n",
    "    pickle.dump(dic, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[barcode for barcode in dic if barcode in big_words_dict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "363547\n",
      "375415\n"
     ]
    }
   ],
   "source": [
    "print(len(big_words_dict))\n",
    "for barcode in dic:\n",
    "    big_words_dict[barcode] = dic[barcode]\n",
    "print(len(big_words_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('barcodes_dict_with_new_ones.pkl', 'wb') as file:\n",
    "    # A new file will be created\n",
    "    pickle.dump(big_words_dict, file)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7ba1f239449f425eb3a38518ae99d1f767aed482cc07821544aaa5748f4ec966"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('env_tf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
