{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "064e2d60",
   "metadata": {},
   "source": [
    "# INDEX\n",
    "* [Imports and functions](#Imports-and-functions)\n",
    "* [Configuration](#Configuration)\n",
    "* [Prepare dataset](#Prepare-dataset)\n",
    "* [Build model](#Build-model)\n",
    "    * [Model inputs](#Model-inputs)\n",
    "    * [Model output](#Model-output)\n",
    "    * [Model](#Model)\n",
    "* [Train model](#Train-model)\n",
    "    * [Training stats](#Training-stats)\n",
    "* [Save model and resources](#Save-model-and-resources)\n",
    "* [Test model](#Test-model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c9e5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  import google.colab\n",
    "  IN_COLAB = True\n",
    "except:\n",
    "  IN_COLAB = False\n",
    "# eventual initialization for colab notebooks\n",
    "if IN_COLAB:\n",
    "  # we try hard to be re-entrant,\n",
    "  # that is to be able to rerun this without cloning repository more than once\n",
    "  COLAB_BRANCH = \"master\"\n",
    "  !curl https://raw.githubusercontent.com/openfoodfacts/off-category-classification/$COLAB_BRANCH/lib/colab.py --output /content/colab.py\n",
    "  !cd /content && python /content/colab.py $COLAB_BRANCH\n",
    "  %cd /content/off-category-classification/experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bcfaa2d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# codecarbon - start tracking\n",
    "from codecarbon import EmissionsTracker\n",
    "\n",
    "tracker = EmissionsTracker(log_level=\"WARNING\", save_to_api=True, experiment_id=\"6d2c8401-afba-42de-9600-6e95bea5fd80\")\n",
    "tracker.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d481d404",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c4f3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "PROJECT_DIR = Path(\"..\").absolute().resolve()\n",
    "sys.path.append(str(PROJECT_DIR)) # append a relative path to the top package to the search path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda92a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.keras import callbacks, layers\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "from lib.dataset import *\n",
    "from lib.directories import init_cache_dir, init_model_dir\n",
    "from lib.io import load_model, save_model\n",
    "from lib.model import top_labeled_predictions, top_predictions_table\n",
    "from lib.plot import plot_training_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1bfc7ed",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92c3dd7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "MODEL_BASE_DIR = PROJECT_DIR / \"model\"\n",
    "CACHE_DIR = PROJECT_DIR / \"tensorflow_cache\"\n",
    "\n",
    "PREPROC_BATCH_SIZE = 10_000  # some large value, only affects execution time\n",
    "\n",
    "# splits are handled by `tfds.load`, see doc for more elaborate ways to sample\n",
    "TRAIN_SPLIT = 'train[:80%]'\n",
    "VAL_SPLIT = 'train[80%:90%]'\n",
    "TEST_SPLIT = 'train[90%:]'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24785109",
   "metadata": {},
   "source": [
    "# Prepare dataset\n",
    "\n",
    "Run this once to fetch, build and cache the dataset.\n",
    "Further runs will be no-ops, unless you force operations (see TFDS doc).\n",
    "\n",
    "Once this is done, `load_dataset('off_categories', ...)` to access the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda1dad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets.off_categories\n",
    "\n",
    "builder = tfds.builder('off_categories')\n",
    "builder.download_and_prepare()\n",
    "\n",
    "# Or run via command line (if `tfds` is in the path):\n",
    "# !cd ../datasets && tfds build off_categories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb92b58e",
   "metadata": {},
   "source": [
    "# Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee58091",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993bb1d8",
   "metadata": {},
   "source": [
    "## Model inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531185de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use dicts so rerunning individual model cells is idempotent\n",
    "inputs = {}\n",
    "input_graphs = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8353a5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset('off_categories', split=TRAIN_SPLIT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61fec40",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "feature_name = 'product_name'\n",
    "\n",
    "product_name_input = tf.keras.Input(shape=(1,), dtype=tf.string, name=feature_name)\n",
    "\n",
    "product_name_vectorizer = layers.TextVectorization(\n",
    "    split = 'whitespace',\n",
    "    max_tokens = 93_000,\n",
    "    output_sequence_length = 30)\n",
    "\n",
    "product_name_vectorizer.adapt(\n",
    "    select_feature(ds, feature_name).batch(PREPROC_BATCH_SIZE))\n",
    "\n",
    "x = product_name_vectorizer(product_name_input)\n",
    "\n",
    "x = layers.Embedding(\n",
    "    input_dim = product_name_vectorizer.vocabulary_size(),\n",
    "    output_dim = 64,\n",
    "    mask_zero = False)(x)\n",
    "\n",
    "product_name_graph = layers.Bidirectional(layers.LSTM(\n",
    "    units = 64,\n",
    "    recurrent_dropout = 0.2,\n",
    "    dropout = 0.0))(x)\n",
    "\n",
    "inputs[feature_name] = product_name_input\n",
    "input_graphs[feature_name] = product_name_graph\n",
    "\n",
    "len(product_name_vectorizer.get_vocabulary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb0a17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "feature_name = 'ingredients_tags'\n",
    "\n",
    "ingredients_input = tf.keras.Input(shape=(None,), dtype=tf.string, name=feature_name)\n",
    "\n",
    "ingredients_vocab = get_vocabulary(\n",
    "    flat_batch(select_feature(ds, feature_name), batch_size=PREPROC_BATCH_SIZE),\n",
    "    min_freq = 3,\n",
    "    max_tokens = 5_000)\n",
    "\n",
    "ingredients_graph = layers.StringLookup(\n",
    "    vocabulary = ingredients_vocab,\n",
    "    output_mode = 'multi_hot')(ingredients_input)\n",
    "\n",
    "inputs[feature_name] = ingredients_input\n",
    "input_graphs[feature_name] = ingredients_graph\n",
    "\n",
    "len(ingredients_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8a30f1",
   "metadata": {},
   "source": [
    "## Model output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3426e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "labels = 'categories_tags'\n",
    "\n",
    "categories_vocab = get_vocabulary(\n",
    "    flat_batch(select_feature(ds, labels), batch_size=PREPROC_BATCH_SIZE),\n",
    "    min_freq = 10)\n",
    "\n",
    "# StringLookup(output_mode='multi_hot') mode requires num_oov_indices >= 1.\n",
    "# We don't want OOVs in the categories_tags output layer, since it wouldn't make\n",
    "# sense to predict OOV. So we'll drop the OOV in _transform below.\n",
    "# Be careful when using StringLookup methods, some of them will return values\n",
    "# based on a vocabulary with OOV (e.g. vocabulary_size()). Keep this in mind when\n",
    "# mapping predictions back to the original vocabulary.\n",
    "categories_multihot = layers.StringLookup(\n",
    "    vocabulary = categories_vocab,\n",
    "    output_mode = 'multi_hot',\n",
    "    num_oov_indices = 1)\n",
    "\n",
    "def categories_encode(ds: tf.data.Dataset):\n",
    "    @tf.function\n",
    "    @tf.autograph.experimental.do_not_convert\n",
    "    def _transform(x, y):\n",
    "        y = categories_multihot(y)\n",
    "        y = y[1:]  # drop OOV\n",
    "        return (x, y)\n",
    "\n",
    "    # applies to non-batched dataset\n",
    "    return (\n",
    "        ds\n",
    "        .map(_transform, num_parallel_calls=tf.data.AUTOTUNE, deterministic=True)\n",
    "        .apply(filter_empty_labels)\n",
    "    )\n",
    "\n",
    "len(categories_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3c2786",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b498ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure final order is independent of cell execution/insertion order\n",
    "features = sorted(inputs.keys())\n",
    "\n",
    "x = layers.Concatenate()([input_graphs[k] for k in features])\n",
    "x = layers.Dropout(0.2)(x)\n",
    "x = layers.Dense(64)(x)\n",
    "x = layers.Dropout(0.2)(x)\n",
    "x = layers.Activation('relu')(x)\n",
    "output = layers.Dense(len(categories_vocab), activation='sigmoid')(x)\n",
    "\n",
    "model = tf.keras.Model(inputs=[inputs[k] for k in features], outputs=[output])\n",
    "\n",
    "threshold = 0.5\n",
    "num_labels = len(categories_vocab)\n",
    "\n",
    "model.compile(\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss = tf.keras.losses.BinaryCrossentropy(label_smoothing=0.0),\n",
    "    metrics = [\n",
    "        tf.metrics.Precision(thresholds=threshold, name='precision'),\n",
    "        tf.metrics.Recall(thresholds=threshold, name='recall'),\n",
    "        tfa.metrics.F1Score(average='micro', threshold=threshold, num_classes=num_labels, name='f1_score_micro'),\n",
    "        tfa.metrics.F1Score(average='macro', threshold=threshold, num_classes=num_labels, name='f1_score_macro')\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5df8ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec2d376",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model, show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3f44b0",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53441242",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Remember to clean obsolete dirs once in a while\n",
    "MODEL_DIR = init_model_dir(MODEL_BASE_DIR)\n",
    "CACHE_DIR = init_cache_dir(CACHE_DIR)\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "ds_train = (\n",
    "    load_dataset('off_categories', split=TRAIN_SPLIT, features=features, as_supervised=True)\n",
    "    .apply(categories_encode)\n",
    "    .padded_batch(batch_size)\n",
    "    .cache(str(CACHE_DIR / 'train'))\n",
    ")\n",
    "\n",
    "ds_val = (\n",
    "    load_dataset('off_categories', split=VAL_SPLIT, features=features, as_supervised=True)\n",
    "    .apply(categories_encode)\n",
    "    .padded_batch(batch_size)\n",
    "    .cache(str(CACHE_DIR / 'val'))\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    ds_train,\n",
    "    epochs = 50,\n",
    "    validation_data = ds_val,\n",
    "    callbacks = [\n",
    "        callbacks.TerminateOnNaN(),\n",
    "        callbacks.ModelCheckpoint(\n",
    "            filepath = str(MODEL_DIR / \"weights.{epoch:02d}-{val_loss:.4f}\"),\n",
    "            monitor = 'val_loss',\n",
    "            save_best_only = True,\n",
    "            save_format = 'tf',\n",
    "        ),\n",
    "        callbacks.EarlyStopping(monitor='val_loss', patience=4),\n",
    "        callbacks.CSVLogger(str(MODEL_DIR / 'training.log')),\n",
    "        callbacks.History()\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ebb0214",
   "metadata": {},
   "source": [
    "## Training stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab63d125",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = pd.read_csv(MODEL_DIR / 'training.log')\n",
    "stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6244c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_stats(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d157d84",
   "metadata": {},
   "source": [
    "# Save model and resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441077ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVED_MODEL_DIR = MODEL_DIR / 'saved_model'\n",
    "\n",
    "@tf.function\n",
    "def serving_func(*args, **kwargs):\n",
    "    preds = model(*args, **kwargs)\n",
    "    return top_labeled_predictions(preds, categories_vocab, k=50)\n",
    "\n",
    "save_model(SAVED_MODEL_DIR, model, categories_vocab, serving_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a29603",
   "metadata": {},
   "source": [
    "# Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdba60dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "m, labels = load_model(SAVED_MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff4364b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_test = load_dataset('off_categories', split=TEST_SPLIT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df9fcc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "preds_test = m.predict(ds_test.padded_batch(128))\n",
    "preds_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad3cac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the function exported as the default serving function in our saved model\n",
    "top_preds_test = top_labeled_predictions(preds_test, labels, k=3)\n",
    "top_preds_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f3aad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same data, but pretty\n",
    "pred_table_test = top_predictions_table(top_preds_test)\n",
    "\n",
    "# Add some interpretable features to the final table\n",
    "# Table must be row-aligned with predictions above (= taken from same data sample)\n",
    "extra_cols_test = as_dataframe(select_features(ds_test, ['code', 'product_name']))\n",
    "\n",
    "pd.concat([extra_cols_test, pred_table_test], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2d9a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# codecarbon - stop tracking\n",
    "tracker.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
