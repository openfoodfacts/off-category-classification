{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "064e2d60",
   "metadata": {},
   "source": [
    "# INDEX\n",
    "* [Imports and functions](#Imports-and-functions)\n",
    "* [Configuration](#Configuration)\n",
    "* [Prepare dataset](#Prepare-dataset)\n",
    "* [Build model](#Build-model)\n",
    "    * [Model inputs](#Model-inputs)\n",
    "    * [Model output](#Model-output)\n",
    "    * [Model](#Model)\n",
    "* [Train model](#Train-model)\n",
    "    * [Training stats](#Training-stats)\n",
    "* [Save model and resources](#Save-model-and-resources)\n",
    "* [Test model](#Test-model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c5cbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66695f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c9e5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  import google.colab\n",
    "  IN_COLAB = True\n",
    "except:\n",
    "  IN_COLAB = False\n",
    "# eventual initialization for colab notebooks\n",
    "if IN_COLAB:\n",
    "  # we try hard to be re-entrant,\n",
    "  # that is to be able to rerun this without cloning repository more than once\n",
    "  COLAB_BRANCH = \"master\"\n",
    "  !curl https://raw.githubusercontent.com/openfoodfacts/off-category-classification/$COLAB_BRANCH/lib/colab.py --output /content/colab.py\n",
    "  !cd /content && python /content/colab.py $COLAB_BRANCH\n",
    "  %cd /content/off-category-classification/experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bcfaa2d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# codecarbon - start tracking\n",
    "from codecarbon import EmissionsTracker\n",
    "\n",
    "tracker = EmissionsTracker(\n",
    "    log_level=\"WARNING\",\n",
    "    save_to_api=True,\n",
    "    experiment_id=\"6d2c8401-afba-42de-9600-6e95bea5fd80\",\n",
    ")\n",
    "tracker.start()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d481d404",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c4f3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "PROJECT_DIR = Path(\"..\").absolute().resolve()\n",
    "sys.path.append(\n",
    "    str(PROJECT_DIR)\n",
    ")  # append a relative path to the top package to the search path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda92a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataclasses\n",
    "import datetime\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.keras import callbacks, layers\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "from lib.dataset import (\n",
    "    as_dataframe,\n",
    "    filter_empty_labels,\n",
    "    flat_batch,\n",
    "    get_vocabulary,\n",
    "    load_dataset,\n",
    "    select_feature,\n",
    "    select_features,\n",
    ")\n",
    "from lib.directories import init_model_dir\n",
    "from lib.io import load_model, save_model\n",
    "from lib.metrics import PrecisionWithAverage, RecallWithAverage\n",
    "from lib.model import top_labeled_predictions, top_predictions_table\n",
    "from lib.plot import plot_training_stats\n",
    "from lib.taxonomy import get_taxonomy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1bfc7ed",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92c3dd7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "MODEL_BASE_DIR = PROJECT_DIR / \"model\"\n",
    "\n",
    "PREPROC_BATCH_SIZE = 10_000  # some large value, only affects execution time\n",
    "\n",
    "# splits are handled by `tfds.load`, see doc for more elaborate ways to sample\n",
    "TRAIN_SPLIT = \"train[:80%]\"\n",
    "VAL_SPLIT = \"train[80%:90%]\"\n",
    "TEST_SPLIT = \"train[90%:]\"\n",
    "\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class Config:\n",
    "    mixed_precision: bool = False\n",
    "    random_seed: int = 42\n",
    "\n",
    "\n",
    "CONFIG = Config(mixed_precision=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24785109",
   "metadata": {},
   "source": [
    "# Prepare dataset\n",
    "\n",
    "Run this once to fetch, build and cache the dataset.\n",
    "Further runs will be no-ops, unless you force operations (see TFDS doc).\n",
    "\n",
    "Once this is done, `load_dataset('off_categories', ...)` to access the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f8bb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_taxonomy = get_taxonomy(\"category\", offline=True)\n",
    "ingredient_taxonomy = get_taxonomy(\"ingredient\", offline=True)\n",
    "print(f\"{len(ingredient_taxonomy)=}, {len(category_taxonomy)=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda1dad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets.off_categories\n",
    "\n",
    "builder = tfds.builder(\"off_categories\")\n",
    "builder.download_and_prepare()\n",
    "\n",
    "# Or run via command line (if `tfds` is in the path):\n",
    "# !cd ../datasets && tfds build off_categories\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb92b58e",
   "metadata": {},
   "source": [
    "# Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee58091",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_random_seed(seed):\n",
    "    tf.random.set_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "\n",
    "set_random_seed(CONFIG.random_seed)\n",
    "\n",
    "if CONFIG.mixed_precision:\n",
    "    print(\"Using mixed precision\")\n",
    "    tf.keras.mixed_precision.set_global_policy(\"mixed_float16\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993bb1d8",
   "metadata": {},
   "source": [
    "## Model inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531185de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use dicts so rerunning individual model cells is idempotent\n",
    "inputs = {}\n",
    "input_graphs = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8353a5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"off_categories\", split=TRAIN_SPLIT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61fec40",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "feature_name = \"product_name\"\n",
    "\n",
    "product_name_input = tf.keras.Input(shape=(1,), dtype=tf.string, name=feature_name)\n",
    "\n",
    "product_name_vectorizer = layers.TextVectorization(\n",
    "    split=\"whitespace\", max_tokens=93_000, output_sequence_length=30\n",
    ")\n",
    "\n",
    "product_name_vectorizer.adapt(\n",
    "    select_feature(ds, feature_name).batch(PREPROC_BATCH_SIZE)\n",
    ")\n",
    "\n",
    "x = product_name_vectorizer(product_name_input)\n",
    "\n",
    "x = layers.Embedding(\n",
    "    input_dim=product_name_vectorizer.vocabulary_size(), output_dim=64, mask_zero=False\n",
    ")(x)\n",
    "\n",
    "product_name_graph = layers.Bidirectional(\n",
    "    layers.LSTM(units=64, recurrent_dropout=0.0, dropout=0.2)\n",
    ")(x)\n",
    "\n",
    "inputs[feature_name] = product_name_input\n",
    "input_graphs[feature_name] = product_name_graph\n",
    "\n",
    "len(product_name_vectorizer.get_vocabulary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb0a17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "feature_name = \"ingredients_tags\"\n",
    "\n",
    "ingredients_input = tf.keras.Input(shape=(None,), dtype=tf.string, name=feature_name)\n",
    "\n",
    "ingredients_vocab = get_vocabulary(\n",
    "    flat_batch(select_feature(ds, feature_name), batch_size=PREPROC_BATCH_SIZE),\n",
    "    min_freq=3,\n",
    "    max_tokens=5_000,\n",
    "    add_pad_token=True,\n",
    "    add_oov_token=True,\n",
    ")\n",
    "\n",
    "ingredients_lookup_layer = layers.StringLookup(vocabulary=ingredients_vocab, num_oov_indices=1, output_mode=\"int\", mask_token=\"\")\n",
    "x = ingredients_lookup_layer(ingredients_input)\n",
    "x = layers.Embedding(\n",
    "    input_dim=ingredients_lookup_layer.vocabulary_size(), output_dim=64, mask_zero=True\n",
    ")(x)\n",
    "ingredients_graph = layers.Bidirectional(\n",
    "    layers.LSTM(units=64, recurrent_dropout=0.0, dropout=0.2)\n",
    ")(x)\n",
    "\n",
    "inputs[feature_name] = ingredients_input\n",
    "input_graphs[feature_name] = ingredients_graph\n",
    "\n",
    "len(ingredients_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8a30f1",
   "metadata": {},
   "source": [
    "## Model output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3426e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "labels = \"categories_tags\"\n",
    "\n",
    "categories_vocab = get_vocabulary(\n",
    "    flat_batch(select_feature(ds, labels), batch_size=PREPROC_BATCH_SIZE), min_freq=10\n",
    ")\n",
    "\n",
    "# StringLookup(output_mode='multi_hot') mode requires num_oov_indices >= 1.\n",
    "# We don't want OOVs in the categories_tags output layer, since it wouldn't make\n",
    "# sense to predict OOV. So we'll drop the OOV in _transform below.\n",
    "# Be careful when using StringLookup methods, some of them will return values\n",
    "# based on a vocabulary with OOV (e.g. vocabulary_size()). Keep this in mind when\n",
    "# mapping predictions back to the original vocabulary.\n",
    "categories_multihot = layers.StringLookup(\n",
    "    vocabulary=categories_vocab, output_mode=\"multi_hot\", num_oov_indices=1\n",
    ")\n",
    "\n",
    "\n",
    "def categories_encode(ds: tf.data.Dataset):\n",
    "    @tf.function\n",
    "    @tf.autograph.experimental.do_not_convert\n",
    "    def _transform(x, y):\n",
    "        y = categories_multihot(y)\n",
    "        y = y[1:]  # drop OOV\n",
    "        return (x, y)\n",
    "\n",
    "    # applies to non-batched dataset\n",
    "    return ds.map(_transform, num_parallel_calls=tf.data.AUTOTUNE, deterministic=True)\n",
    "\n",
    "\n",
    "len(categories_vocab)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3c2786",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b498ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure final order is independent of cell execution/insertion order\n",
    "features = sorted(inputs.keys())\n",
    "\n",
    "x = layers.Concatenate()([input_graphs[k] for k in features])\n",
    "x = layers.Dropout(0.2)(x)\n",
    "x = layers.Dense(64)(x)\n",
    "x = layers.Dropout(0.2)(x)\n",
    "x = layers.Activation(\"relu\")(x)\n",
    "output = layers.Dense(len(categories_vocab), activation=\"sigmoid\")(x)\n",
    "\n",
    "model = tf.keras.Model(inputs=[inputs[k] for k in features], outputs=[output])\n",
    "\n",
    "threshold = 0.5\n",
    "num_labels = len(categories_vocab)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(label_smoothing=0.0),\n",
    "    metrics=[\n",
    "        PrecisionWithAverage(\n",
    "            average=\"micro\",\n",
    "            threshold=threshold,\n",
    "            num_classes=num_labels,\n",
    "            name=\"precision_micro\",\n",
    "        ),\n",
    "        PrecisionWithAverage(\n",
    "            average=\"macro\",\n",
    "            threshold=threshold,\n",
    "            num_classes=num_labels,\n",
    "            name=\"precision_macro\",\n",
    "        ),\n",
    "        RecallWithAverage(\n",
    "            average=\"micro\",\n",
    "            threshold=threshold,\n",
    "            num_classes=num_labels,\n",
    "            name=\"recall_micro\",\n",
    "        ),\n",
    "        RecallWithAverage(\n",
    "            average=\"macro\",\n",
    "            threshold=threshold,\n",
    "            num_classes=num_labels,\n",
    "            name=\"recall_macro\",\n",
    "        ),\n",
    "        tfa.metrics.F1Score(\n",
    "            average=\"micro\",\n",
    "            threshold=threshold,\n",
    "            num_classes=num_labels,\n",
    "            name=\"f1_score_micro\",\n",
    "        ),\n",
    "        tfa.metrics.F1Score(\n",
    "            average=\"macro\",\n",
    "            threshold=threshold,\n",
    "            num_classes=num_labels,\n",
    "            name=\"f1_score_macro\",\n",
    "        ),\n",
    "    ],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5df8ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec2d376",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model, show_shapes=True, show_layer_names=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3f44b0",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53441242",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Remember to clean obsolete dirs once in a while\n",
    "MODEL_DIR = init_model_dir(MODEL_BASE_DIR)\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "ds_train = (\n",
    "    load_dataset(\n",
    "        \"off_categories\", split=TRAIN_SPLIT, features=features, as_supervised=True\n",
    "    )\n",
    "    .apply(categories_encode)\n",
    "    .padded_batch(batch_size)\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")\n",
    "\n",
    "ds_val = (\n",
    "    load_dataset(\n",
    "        \"off_categories\", split=VAL_SPLIT, features=features, as_supervised=True\n",
    "    )\n",
    "    .apply(categories_encode)\n",
    "    .padded_batch(batch_size)\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    ds_train,\n",
    "    epochs=50,\n",
    "    validation_data=ds_val,\n",
    "    callbacks=[\n",
    "        callbacks.TerminateOnNaN(),\n",
    "        callbacks.ModelCheckpoint(\n",
    "            filepath=str(MODEL_DIR / \"weights.{epoch:02d}-{val_loss:.4f}\"),\n",
    "            monitor=\"f1_score_micro\",\n",
    "            save_best_only=True,\n",
    "            save_format=\"tf\",\n",
    "        ),\n",
    "        callbacks.CSVLogger(str(MODEL_DIR / \"training.log\")),\n",
    "        callbacks.History(),\n",
    "        callbacks.TensorBoard(\n",
    "            log_dir=\"{}/logs\".format(MODEL_DIR),\n",
    "            write_graph=False,\n",
    "        ),\n",
    "    ],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d157d84",
   "metadata": {},
   "source": [
    "# Save model and resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441077ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVED_MODEL_DIR = MODEL_DIR / \"saved_model\"\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def serving_func(*args, **kwargs):\n",
    "    preds = model(*args, **kwargs)\n",
    "    return top_labeled_predictions(preds, categories_vocab, k=len(categories_vocab))\n",
    "\n",
    "\n",
    "save_model(SAVED_MODEL_DIR, model, categories_vocab, serving_func)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a29603",
   "metadata": {},
   "source": [
    "# Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdba60dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "m, labels = load_model(\n",
    "    SAVED_MODEL_DIR,\n",
    "    custom_objects={\n",
    "        \"PrecisionWithAverage\": PrecisionWithAverage,\n",
    "        \"RecallWithAverage\": RecallWithAverage,\n",
    "    },\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff4364b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_test = load_dataset(\"off_categories\", split=TEST_SPLIT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df9fcc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "preds_test = m.predict(ds_test.padded_batch(128))\n",
    "preds_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46de7b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_test.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad3cac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the function exported as the default serving function in our saved model\n",
    "top_preds_test = top_labeled_predictions(preds_test, labels, k=10)\n",
    "top_preds_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f3aad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same data, but pretty\n",
    "pred_table_test = top_predictions_table(top_preds_test)\n",
    "\n",
    "# Add some interpretable features to the final table\n",
    "# Table must be row-aligned with predictions above (= taken from same data sample)\n",
    "extra_cols_test = as_dataframe(select_features(ds_test, [\"code\", \"product_name\"]))\n",
    "\n",
    "pd.concat([extra_cols_test, pred_table_test], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2d9a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# codecarbon - stop tracking\n",
    "tracker.stop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6 (main, Nov 14 2022, 16:10:14) [GCC 11.3.0]"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "vscode": {
   "interpreter": {
    "hash": "a3dabddd0fe730afce39ea7a97dccf16efaedf3164c7645f99c693ada93b6488"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
