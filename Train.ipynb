{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d481d404",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fda92a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load libtrain.py\n",
    "\n",
    "import argparse\n",
    "import datetime\n",
    "import functools\n",
    "import json\n",
    "import pathlib\n",
    "import shutil\n",
    "import tempfile\n",
    "from typing import Dict, List\n",
    "\n",
    "import dacite\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from robotoff.taxonomy import Taxonomy\n",
    "from tensorflow import keras\n",
    "from tensorflow.data import Dataset\n",
    "from tensorflow.keras import callbacks\n",
    "from tensorflow.python.ops import summary_ops_v2\n",
    "\n",
    "import settings\n",
    "from category_classification.data_utils import (\n",
    "    TFTransformer,\n",
    "    create_tf_dataset,\n",
    "    load_dataframe,\n",
    ")\n",
    "from category_classification.models import (\n",
    "    KerasPreprocessing,\n",
    "    build_model,\n",
    "    construct_preprocessing,\n",
    "    to_serving_model,\n",
    ")\n",
    "\n",
    "from category_classification.config import Config\n",
    "\n",
    "from utils.io import (\n",
    "    copy_category_taxonomy,\n",
    "    save_category_vocabulary,\n",
    "    save_config,\n",
    "    save_json,\n",
    ")\n",
    "from utils.metrics import evaluation_report\n",
    "\n",
    "\n",
    "def create_model(config: Config, preprocess: KerasPreprocessing) -> keras.Model:\n",
    "    model = build_model(config.model_config, preprocess)\n",
    "    loss_fn = keras.losses.BinaryCrossentropy(\n",
    "        label_smoothing=config.train_config.label_smoothing\n",
    "    )\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=config.train_config.lr)\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=loss_fn,\n",
    "        metrics=[\"binary_accuracy\", \"Precision\", \"Recall\"],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_config(args) -> Config:\n",
    "    with args.config.open(\"r\") as f:\n",
    "        config_dict = json.load(f)\n",
    "\n",
    "    print(\"Full configuration:\\n{}\".format(json.dumps(config_dict, indent=4)))\n",
    "    return dacite.from_dict(Config, config_dict)\n",
    "\n",
    "\n",
    "class TBCallback(callbacks.TensorBoard):\n",
    "    \"\"\"Get around a bug where you cannot use the TensorBoard callback with the StringLookup layers\n",
    "    - https://github.com/tensorflow/tensorboard/issues/4530#issuecomment-783318292\"\"\"\n",
    "\n",
    "    def _log_weights(self, epoch):\n",
    "        with self._train_writer.as_default():\n",
    "            with summary_ops_v2.always_record_summaries():\n",
    "                for layer in self.model.layers:\n",
    "                    for weight in layer.weights:\n",
    "                        if hasattr(weight, \"name\"):\n",
    "                            weight_name = weight.name.replace(\":\", \"_\")\n",
    "                            summary_ops_v2.histogram(weight_name, weight, step=epoch)\n",
    "                            if self.write_images:\n",
    "                                self._log_weight_as_image(weight, weight_name, epoch)\n",
    "                self._train_writer.flush()\n",
    "\n",
    "\n",
    "def train(\n",
    "    model: keras.Model,\n",
    "    save_dir: pathlib.Path,\n",
    "    config: Config,\n",
    "    category_vocab: List[str],\n",
    "):\n",
    "    print(\"Starting training...\")\n",
    "    temporary_log_dir = pathlib.Path(tempfile.mkdtemp())\n",
    "    print(\"Temporary log directory: {}\".format(temporary_log_dir))\n",
    "\n",
    "    tf_transformer = TFTransformer(category_vocab)\n",
    "\n",
    "    train = create_tf_dataset(\"train\", config.train_config.batch_size, tf_transformer)\n",
    "    val = create_tf_dataset(\"val\", config.train_config.batch_size, tf_transformer)\n",
    "\n",
    "    model.fit(train,\n",
    "        epochs= config.train_config.epochs,\n",
    "        validation_data=val,\n",
    "        callbacks=[\n",
    "            callbacks.TerminateOnNaN(),\n",
    "            callbacks.ModelCheckpoint(\n",
    "                filepath=str(save_dir / \"weights.{epoch:02d}-{val_loss:.4f}\"),\n",
    "                monitor=\"val_loss\",\n",
    "                save_best_only=True,\n",
    "                save_format='tf',\n",
    "            ),\n",
    "            TBCallback(log_dir=str(temporary_log_dir), histogram_freq=1),\n",
    "            callbacks.EarlyStopping(monitor=\"val_loss\", patience=4),\n",
    "            callbacks.CSVLogger(str(save_dir / \"training.csv\")),\n",
    "        ],\n",
    "    )\n",
    "    print(\"Training ended\")\n",
    "\n",
    "    log_dir = save_dir / \"logs\"\n",
    "    print(\"Moving log directory from {} to {}\".format(temporary_log_dir, log_dir))\n",
    "    shutil.move(str(temporary_log_dir), str(log_dir))\n",
    "\n",
    "    print(\"Saving the base and the serving model {}\".format(save_dir))\n",
    "    model.save(str(save_dir / \"base/saved_model\"))\n",
    "    to_serving_model(model, category_vocab).save(str(save_dir / \"serving/saved_model\"))\n",
    "\n",
    "    category_taxonomy = Taxonomy.from_json(settings.CATEGORY_TAXONOMY_PATH)\n",
    "\n",
    "    print(\"Evaluating on validation dataset\")\n",
    "    y_pred_val = model.predict(val.map(lambda x,y: x))\n",
    "    report, clf_report = evaluation_report(\n",
    "        val.map(lambda x, y: y).as_numpy(), y_pred_val, taxonomy=category_taxonomy, category_names=category_names\n",
    "    )\n",
    "\n",
    "    save_json(report, save_dir / \"metrics_val.json\")\n",
    "    save_json(clf_report, save_dir / \"classification_report_val.json\")\n",
    "\n",
    "    print(\"Evaluating on test dataset\")\n",
    "    test = create_tf_dataset(\"test\", config.train_config.batch_size, tf_transformer)\n",
    "    y_pred_test = model.predict(test.map(lambda x,y: x))\n",
    "    report, clf_report = evaluation_report(\n",
    "        test.map(lambda x, y: y).as_numpy, y_pred_test, taxonomy=category_taxonomy, category_names=category_names\n",
    "    )\n",
    "\n",
    "    save_json(report, save_dir / \"metrics_test.json\")\n",
    "    save_json(clf_report, save_dir / \"classification_report_test.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1bfc7ed",
   "metadata": {},
   "source": [
    "# JSON Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a92c3dd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train_config': {'batch_size': 128,\n",
       "  'epochs': 50,\n",
       "  'lr': 0.001,\n",
       "  'label_smoothing': 0.0},\n",
       " 'model_config': {'product_name_lstm_recurrent_dropout': 0.2,\n",
       "  'product_name_lstm_dropout': 0.0,\n",
       "  'product_name_embedding_size': 64,\n",
       "  'product_name_lstm_units': 64,\n",
       "  'product_name_max_length': 30,\n",
       "  'product_name_max_tokens': 93000,\n",
       "  'hidden_dim': 64,\n",
       "  'hidden_dropout': 0.2,\n",
       "  'category_min_count': 10,\n",
       "  'ingredient_min_count': 3}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load config json\n",
    "import json\n",
    " \n",
    "# Opening JSON file\n",
    "with open('config.json') as json_file:\n",
    "    json_config = json.load(json_file)\n",
    "json_config "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba2de6e",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bdb0c0a0",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [22]\u001b[0m, in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m output_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodels\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      7\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(output_dir, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m----> 9\u001b[0m keras_preprocess \u001b[38;5;241m=\u001b[39m \u001b[43mconstruct_preprocessing\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcategory_min_count\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mingredient_min_count\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mproduct_name_max_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mproduct_name_max_length\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_dataframe\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPre-processed training data\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     18\u001b[0m replicates \u001b[38;5;241m=\u001b[39m args\u001b[38;5;241m.\u001b[39mrepeat\n",
      "File \u001b[0;32m~/Documents/Projet/d4goff/off-category-classification/category_classification/models.py:97\u001b[0m, in \u001b[0;36mconstruct_preprocessing\u001b[0;34m(category_min_count, ingredients_min_count, max_product_name_tokens, max_product_name_length, train_df)\u001b[0m\n\u001b[1;32m     88\u001b[0m category_lookup \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mStringLookup(\n\u001b[1;32m     89\u001b[0m     vocabulary\u001b[38;5;241m=\u001b[39mcat_vocab, output_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulti_hot\u001b[39m\u001b[38;5;124m\"\u001b[39m, num_oov_indices\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     90\u001b[0m )\n\u001b[1;32m     92\u001b[0m product_name_preprocessing \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mTextVectorization(\n\u001b[1;32m     93\u001b[0m     split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhitespace\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     94\u001b[0m     max_tokens\u001b[38;5;241m=\u001b[39mmax_product_name_tokens,\n\u001b[1;32m     95\u001b[0m     output_sequence_length\u001b[38;5;241m=\u001b[39mmax_product_name_length,\n\u001b[1;32m     96\u001b[0m )\n\u001b[0;32m---> 97\u001b[0m \u001b[43mproduct_name_preprocessing\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madapt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproduct_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     99\u001b[0m ingredient_vocab \u001b[38;5;241m=\u001b[39m _construct_preprocessing_vocab(\n\u001b[1;32m    100\u001b[0m     train_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mknown_ingredient_tags\u001b[39m\u001b[38;5;124m\"\u001b[39m], ingredients_min_count\n\u001b[1;32m    101\u001b[0m )\n\u001b[1;32m    102\u001b[0m ingredient_preprocessing \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mStringLookup(\n\u001b[1;32m    103\u001b[0m     vocabulary\u001b[38;5;241m=\u001b[39mingredient_vocab, output_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulti_hot\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    104\u001b[0m )\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/off/lib/python3.8/site-packages/keras/engine/base_preprocessing_layer.py:248\u001b[0m, in \u001b[0;36mPreprocessingLayer.adapt\u001b[0;34m(self, data, batch_size, steps)\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mcatch_stop_iteration():\n\u001b[1;32m    247\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39msteps():\n\u001b[0;32m--> 248\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_adapt_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m    250\u001b[0m       context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/off/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py:885\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    882\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    884\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 885\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    887\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    888\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/off/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py:924\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    921\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    922\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[1;32m    923\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[0;32m--> 924\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stateful_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    925\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ALLOW_DYNAMIC_VARIABLE_CREATION:\n\u001b[1;32m    926\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    927\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/off/lib/python3.8/site-packages/tensorflow/python/eager/function.py:3039\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3036\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m   3037\u001b[0m   (graph_function,\n\u001b[1;32m   3038\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m-> 3039\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3040\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/off/lib/python3.8/site-packages/tensorflow/python/eager/function.py:1963\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1959\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1960\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1961\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1962\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1963\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1964\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1965\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1966\u001b[0m     args,\n\u001b[1;32m   1967\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1968\u001b[0m     executing_eagerly)\n\u001b[1;32m   1969\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/off/lib/python3.8/site-packages/tensorflow/python/eager/function.py:591\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    589\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    590\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 591\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    592\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    593\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    594\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    595\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    596\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    597\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    598\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    599\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[1;32m    600\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    603\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[1;32m    604\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/off/lib/python3.8/site-packages/tensorflow/python/eager/execute.py:59\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     58\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 59\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     62\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "config = json_config\n",
    "model_config = config[\"model_config\"]\n",
    "\n",
    "output_dir = \"models\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "keras_preprocess = construct_preprocessing(\n",
    "    model_config[\"category_min_count\"],\n",
    "    model_config[\"ingredient_min_count\"],\n",
    "    model_config[\"product_name_max_tokens\"],\n",
    "    model_config[\"product_name_max_length\"],\n",
    "    load_dataframe(\"train\"),\n",
    ")\n",
    "print(\"Pre-processed training data\")\n",
    "\n",
    "replicates = args.repeat\n",
    "if replicates == 1:\n",
    "    save_dirs = [output_dir]\n",
    "else:\n",
    "    save_dirs = [output_dir / str(i) for i in range(replicates)]\n",
    "\n",
    "for i, save_dir in enumerate(save_dirs):\n",
    "    model = create_model(config, keras_preprocess)\n",
    "\n",
    "    save_dir.mkdir(exist_ok=True)\n",
    "    config.train_config.start_datetime = str(datetime.datetime.utcnow())\n",
    "    print(f\"Starting training repeat {i}\")\n",
    "\n",
    "    save_config(config, save_dir)\n",
    "    copy_category_taxonomy(settings.CATEGORY_TAXONOMY_PATH, save_dir)\n",
    "    save_category_vocabulary(keras_preprocess.category_vocab, save_dir)\n",
    "\n",
    "    train(\n",
    "        model,\n",
    "        save_dir,\n",
    "        config,\n",
    "        keras_preprocess.category_vocab,\n",
    "    )\n",
    "\n",
    "    config[\"train_config\"][\"end_datetime\"] = str(datetime.datetime.utcnow())\n",
    "    save_config(config, save_dir)\n",
    "    config[\"train_config\"][\"start_datetime\"] = None\n",
    "    config[\"train_config\"][\"end_datetime\"] = None"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
